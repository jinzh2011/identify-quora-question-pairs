{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "from tqdm import tqdm\n",
    "from numpy import random\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from sklearn import pipeline, preprocessing\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict,cross_val_score, StratifiedShuffleSplit,GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, \\\n",
    "                            accuracy_score, f1_score, roc_auc_score, roc_curve, \\\n",
    "                             precision_recall_curve,log_loss, confusion_matrix\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk\n",
    "#from nltk import tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, WhitespaceTokenizer\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from xgboost import XGBClassifier\n",
    "from fuzzywuzzy import fuzz\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "#from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "import gensim\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/quora_duplicate_questions.tsv', sep='\\t')\n",
    "df.fillna('',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X,y  = df[['question1','question2']], df['is_duplicate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_token_features(text1, text2, n_gram):\n",
    "    q1_token = word_tokenize(text1.decode('utf-8'))\n",
    "    q2_token = word_tokenize(text2.decode('utf-8'))\n",
    "    \n",
    "    q1_tags = pos_tag(q1_token)\n",
    "    q2_tags = pos_tag(q2_token)\n",
    "    \n",
    "    q1_noun = [word for word,pos in q1_tags \\\n",
    "        if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS')]\n",
    "    \n",
    "    q2_noun = [word for word,pos in q2_tags \\\n",
    "        if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS')]\n",
    "    \n",
    "    q1_verb = [stemmer.stem(word) for word,pos in q1_tags \\\n",
    "        if (pos == 'VB' or pos == 'VBD' or pos == 'VBG' \\\n",
    "            or pos == 'VBN' or pos == 'VBP' or pos == 'VBZ')]\n",
    "    \n",
    "    q2_verb = [stemmer.stem(word) for word,pos in q2_tags \\\n",
    "        if (pos == 'VB' or pos == 'VBD' or pos == 'VBG' \\\n",
    "            or pos == 'VBN' or pos == 'VBP' or pos == 'VBZ')]\n",
    "    \n",
    "    word_count_avg = (len(q1_token)+len(q2_token))/2.0\n",
    "    word_count_diff = abs(len(q1_token)-len(q2_token))/2.0\n",
    "    word_overlap_count = len(set(q1_token).intersection(set(q2_token)))\n",
    "    word_overlap_ratio = word_overlap_count/word_count_avg if word_count_avg !=0 else 0\n",
    "    \n",
    "    noun_overlap_count = len(set(q1_noun).intersection(set(q2_noun)))\n",
    "    \n",
    "    if (len(q1_noun)+len(q2_noun))/2.0 !=0:\n",
    "        noun_overlap_ratio = noun_overlap_count/((len(q1_noun)+len(q2_noun))/2.0)\n",
    "    else:\n",
    "        noun_overlap_ratio = 1\n",
    "    \n",
    "    verb_overlap_count = len(set(q1_verb).intersection(set(q2_verb)))\n",
    "    \n",
    "    if (len(q1_verb)+len(q2_verb))/2.0 !=0:\n",
    "        verb_overlap_ratio = verb_overlap_count/((len(q1_verb)+len(q2_verb))/2.0)\n",
    "    else:\n",
    "        verb_overlap_ratio = 1\n",
    "    \n",
    "    \n",
    "    return pd.Series({'word_count_avg': word_count_avg,\\\n",
    "                      'word_count_diff':word_count_diff,\\\n",
    "                      'word_overlap_count': word_overlap_count,\\\n",
    "                      'word_overlap_ratio': word_overlap_ratio,\\\n",
    "                      'noun_overlap_count': noun_overlap_count,\\\n",
    "                      'noun_overlap_ratio': noun_overlap_ratio,\\\n",
    "                      'verb_overlap_count': verb_overlap_count,\\\n",
    "                      'verb_overlap_ratio': verb_overlap_ratio\n",
    "                     })\n",
    "\n",
    "token_feature = X.apply(lambda x: get_token_features(x.question1, x.question2, 1), axis=1)    \n",
    "\n",
    "question1_length = X.question1.apply(lambda x: len(x))\n",
    "question2_length = X.question2.apply(lambda x: len(x))\n",
    "length_avg = pd.Series(np.mean([question1_length,question2_length], axis=0),name='length_avg')\n",
    "length_diff = pd.Series(abs(question1_length-question2_length)/2, name='length_diff')\n",
    "length_ratio = pd.Series(length_diff/length_avg, name='length_ratio').fillna(0)\n",
    "\n",
    "same_last_punct = pd.Series((X.question1.apply(lambda x: x[-1] if x != '' else '') == \\\n",
    "                   X.question2.apply(lambda x: x[-1] if x != '' else '')).astype(int), \\\n",
    "                            name='same_last_punct')\n",
    "\n",
    "df_raw = pd.concat([length_avg,length_diff,length_ratio,same_last_punct,\\\n",
    "                       token_feature],\\\n",
    "                      axis=1)\n",
    "\n",
    "df_raw.to_csv('../data/df_raw.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fuzz_features(text1, text2):\n",
    "    fuzz_ratio = fuzz.ratio(text1, text2)\n",
    "    fuzz_partial_ratio = fuzz.partial_ratio(text1, text2)\n",
    "    fuzz_token_sort_ratio  = fuzz.token_sort_ratio(text1, text2)\n",
    "    fuzz_token_set_ratio = fuzz.token_set_ratio(text1, text2)\n",
    "    return pd.Series({'fuzz_ratio': fuzz_ratio,\\\n",
    "                      'fuzz_partial_ratio': fuzz_partial_ratio, \\\n",
    "                      'fuzz_token_sort_ratio': fuzz_token_sort_ratio, \\\n",
    "                      'fuzz_token_set_ratio': fuzz_token_set_ratio\n",
    "                     })\n",
    "df_fuzz = X.apply(lambda x: get_fuzz_features(x.question1, x.question2), axis=1)    \n",
    "df_fuzz.to_csv('../data/df_fuzz.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf_stop = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "\n",
    "tf_matrix = tfidf.fit_transform(np.concatenate([X.question1.values, X.question2.values]))\n",
    "tf_stop_matrix = tfidf_stop.fit_transform(np.concatenate([X.question1.values, X.question2.values]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_reduced_matrix(matrix, prefix, pca_n=50):\n",
    "    lsa = TruncatedSVD(pca_n, algorithm = 'arpack')\n",
    "    reduced_tf_matrix = lsa.fit_transform(matrix)\n",
    "    question1_tf_reduced = reduced_tf_matrix[:X.shape[0],:]\n",
    "    question2_tf_reduced = reduced_tf_matrix[X.shape[0]:,:]\n",
    "    \n",
    "    tf_reduced_cosine_dis = []\n",
    "    tf_reduced_manhattan_dis = []\n",
    "    tf_reduced_euclidean_dis = []\n",
    "    tf_reduced_braycurtis_dis = []\n",
    "\n",
    "    column_name = ['cosine_dis','manhattan_dis','euclidean_dis','braycurtis_dis']\n",
    "    new_column_name = [prefix+'_'+item for item in column_name]\n",
    "    \n",
    "    for i in tqdm(range(X.shape[0])):\n",
    "        tf_reduced_cosine_dis.append(pairwise_distances(question1_tf_reduced[i,:].reshape(1,-1),\\\n",
    "                                                        question2_tf_reduced[i,:].reshape(1,-1), \\\n",
    "                                                        metric='cosine')[0][0])\n",
    "        tf_reduced_manhattan_dis.append(pairwise_distances(question1_tf_reduced[i,:].reshape(1,-1),\\\n",
    "                                                        question2_tf_reduced[i,:].reshape(1,-1), \\\n",
    "                                                        metric='manhattan')[0][0])\n",
    "        tf_reduced_euclidean_dis.append(pairwise_distances(question1_tf_reduced[i,:].reshape(1,-1),\\\n",
    "                                                        question2_tf_reduced[i,:].reshape(1,-1), \\\n",
    "                                                        metric='euclidean')[0][0])\n",
    "        tf_reduced_braycurtis_dis.append(pairwise_distances(question1_tf_reduced[i,:].reshape(1,-1),\\\n",
    "                                                        question2_tf_reduced[i,:].reshape(1,-1), \\\n",
    "                                                        metric='braycurtis')[0][0])\n",
    "\n",
    "    return pd.DataFrame(np.column_stack((tf_reduced_cosine_dis,tf_reduced_manhattan_dis,\\\n",
    "                 tf_reduced_euclidean_dis,tf_reduced_braycurtis_dis)),columns = new_column_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_reduced_tf = get_reduced_matrix(tf_matrix, 'tf_reduced',pca_n= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_reduced_stop_tf = get_reduced_matrix(tf_stop_matrix, 'tf_red_stop',pca_n= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_reduced_tf.to_csv('../data/df_reduced_tf.csv',index=False)\n",
    "df_reduced_stop_tf.to_csv('../data/df_reduced_stop_tf.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tfidf_features(vecor_model, text1, text2, prefix):\n",
    "    tfidf_cosine_dis = pairwise_distances(vecor_model.transform([text1]),\\\n",
    "                                  vecor_model.transform([text2]), metric='cosine')[0][0]\n",
    "    tfidf_manhattan_dis = pairwise_distances(vecor_model.transform([text1]),\\\n",
    "                                 vecor_model.transform([text2]), metric='manhattan')[0][0]\n",
    "    tfidf_euclidean_dis = pairwise_distances(vecor_model.transform([text1]),\\\n",
    "                                  vecor_model.transform([text2]), metric='euclidean')[0][0]\n",
    "    \n",
    "    tfidf_jaccard_dis = pairwise_distances(vecor_model.transform([text1]).todense(),\\\n",
    "                                 vecor_model.transform([text2]).todense(),\\\n",
    "                                           metric='jaccard')[0][0]\n",
    "    tfidf_braycurtis_dis = pairwise_distances(vecor_model.transform([text1]).todense(),\\\n",
    "                                 vecor_model.transform([text2]).todense(),\\\n",
    "                                           metric='braycurtis')[0][0]\n",
    "    \n",
    "    return pd.Series({prefix+'_cosine_dis': tfidf_cosine_dis,\\\n",
    "                      prefix+'_manhattan_dis':tfidf_manhattan_dis,\\\n",
    "                      prefix+'_euclidean_dis':tfidf_euclidean_dis,\\\n",
    "                      prefix+'_jaccard_dis':tfidf_jaccard_dis,\\\n",
    "                      prefix+'_braycurtis_dis':tfidf_braycurtis_dis\n",
    "                     })\n",
    "\n",
    "df_tfidf = X.apply(lambda x: \\\n",
    "                        get_tfidf_features(tfidf, x.question1, x.question2,'tfidf'), axis=1)\n",
    "\n",
    "df_tfidf.to_csv('../data/df_tfidf.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tfidf_stop = X.apply(lambda x: \\\n",
    "                        get_tfidf_features(tfidf_stop, x.question1, x.question2,'tfidf_stop'), axis=1)\n",
    "\n",
    "df_tfidf_stop.to_csv('../data/df_tfidf_stop.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gs_model = gensim.models.KeyedVectors.load_word2vec_format('~/Dropbox/DS/nlp_data/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "\n",
    "#'eury' in gs_model.vocab\n",
    "\n",
    "def sentence_to_vec(sentence):\n",
    "    words = word_tokenize(sentence.decode('utf-8'))\n",
    "    sentence_matrix = []\n",
    "    for w in words:\n",
    "        if w in gs_model.vocab:\n",
    "            sentence_matrix.append(gs_model[w])\n",
    "    if not sentence_matrix:\n",
    "        sentence_vec = np.zeros(300,)\n",
    "    else:\n",
    "        sentence_matrix = np.array(sentence_matrix)\n",
    "        sentence_vec = np.mean(sentence_matrix, axis=0)\n",
    "    \n",
    "    return sentence_vec\n",
    "    \n",
    "\n",
    "def get_sentence2vec_features(text1, text2):\n",
    "    s2vec_cosine_dis = cosine(sentence_to_vec(text1),sentence_to_vec(text2))\n",
    "    s2vec_manhattan_dis = cityblock(sentence_to_vec(text1),sentence_to_vec(text2))\n",
    "    s2vec_canberra_dis = canberra(sentence_to_vec(text1),sentence_to_vec(text2))\n",
    "    s2vec_euclidean_dis = euclidean(sentence_to_vec(text1),sentence_to_vec(text2))\n",
    "    s2vec_braycurtis_dis = braycurtis(sentence_to_vec(text1),sentence_to_vec(text2))\n",
    "    \n",
    "        \n",
    "    return pd.Series({'s2vec_cosine_dis': s2vec_cosine_dis,\\\n",
    "                      's2vec_manhattan_dis':s2vec_manhattan_dis,\\\n",
    "                      's2vec_canberra_dis':s2vec_canberra_dis,\\\n",
    "                      's2vec_euclidean_dis':s2vec_euclidean_dis,\\\n",
    "                      's2vec_braycurtis_dis':s2vec_braycurtis_dis\n",
    "                     })\n",
    "\n",
    "df_s2vec = X.apply(lambda x: get_sentence2vec_features(x.question1, x.question2) , axis=1)\n",
    "\n",
    "df_s2vec = df_s2vec.apply(lambda x: x.fillna(x.max()),axis=0)\n",
    "\n",
    "df_s2vec.to_csv('../data/df_s2vec.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py27",
   "language": "python",
   "name": "py27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
